{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e8408e",
   "metadata": {},
   "source": [
    "## Install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ff1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nic12\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nic12\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'load_ext' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecf097",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a637e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6644a13",
   "metadata": {},
   "source": [
    "## Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50b874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>median_INC_2017-2021</th>\n",
       "      <th>per_capita_INC_2017-2021</th>\n",
       "      <th>major_crime_2018-2022</th>\n",
       "      <th>violent_crime_2018-2022</th>\n",
       "      <th>public_crime_2012-2016</th>\n",
       "      <th>behavioral_ health_hospitalizations_2017</th>\n",
       "      <th>cognitive_difficulty_percent_2017-2021</th>\n",
       "      <th>poverty_rate_percent_2017-2021</th>\n",
       "      <th>high_school_grad_rate_2017-2021</th>\n",
       "      <th>college_grad_rate_2017-2021</th>\n",
       "      <th>non_hispanic_white_percent_2017-2021</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>lead_level</th>\n",
       "      <th>lead_level_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60601</td>\n",
       "      <td>123115.0</td>\n",
       "      <td>137556.0</td>\n",
       "      <td>1736.752366</td>\n",
       "      <td>148.889510</td>\n",
       "      <td>57.189159</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.995805</td>\n",
       "      <td>5.834093</td>\n",
       "      <td>97.917725</td>\n",
       "      <td>88.031149</td>\n",
       "      <td>54.877060</td>\n",
       "      <td>14804.0</td>\n",
       "      <td>14706.9</td>\n",
       "      <td>0.940404</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60602</td>\n",
       "      <td>235666.0</td>\n",
       "      <td>168423.0</td>\n",
       "      <td>282.033895</td>\n",
       "      <td>25.289746</td>\n",
       "      <td>5.774626</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>82.217782</td>\n",
       "      <td>47.022767</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>5350.7</td>\n",
       "      <td>1.464326</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60605</td>\n",
       "      <td>112162.0</td>\n",
       "      <td>85947.0</td>\n",
       "      <td>1675.218975</td>\n",
       "      <td>246.799483</td>\n",
       "      <td>126.443697</td>\n",
       "      <td>285.0</td>\n",
       "      <td>2.914663</td>\n",
       "      <td>8.612424</td>\n",
       "      <td>95.316371</td>\n",
       "      <td>78.088010</td>\n",
       "      <td>57.037753</td>\n",
       "      <td>32077.0</td>\n",
       "      <td>8658.3</td>\n",
       "      <td>0.911358</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60606</td>\n",
       "      <td>130196.0</td>\n",
       "      <td>133605.0</td>\n",
       "      <td>529.774357</td>\n",
       "      <td>58.453847</td>\n",
       "      <td>20.751522</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.363857</td>\n",
       "      <td>8.550637</td>\n",
       "      <td>99.266177</td>\n",
       "      <td>91.794530</td>\n",
       "      <td>59.642207</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>5487.1</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60607</td>\n",
       "      <td>109634.0</td>\n",
       "      <td>65052.0</td>\n",
       "      <td>2208.933790</td>\n",
       "      <td>266.924943</td>\n",
       "      <td>266.380039</td>\n",
       "      <td>651.0</td>\n",
       "      <td>4.155217</td>\n",
       "      <td>16.888971</td>\n",
       "      <td>96.620012</td>\n",
       "      <td>78.920985</td>\n",
       "      <td>51.797266</td>\n",
       "      <td>29990.0</td>\n",
       "      <td>5005.4</td>\n",
       "      <td>1.676312</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60608</td>\n",
       "      <td>61490.0</td>\n",
       "      <td>26958.0</td>\n",
       "      <td>2208.318895</td>\n",
       "      <td>582.919388</td>\n",
       "      <td>978.662767</td>\n",
       "      <td>1137.0</td>\n",
       "      <td>3.625513</td>\n",
       "      <td>16.787975</td>\n",
       "      <td>74.265756</td>\n",
       "      <td>30.445646</td>\n",
       "      <td>20.245194</td>\n",
       "      <td>83689.0</td>\n",
       "      <td>5172.7</td>\n",
       "      <td>3.682467</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60609</td>\n",
       "      <td>43048.0</td>\n",
       "      <td>23009.0</td>\n",
       "      <td>2179.855381</td>\n",
       "      <td>760.609450</td>\n",
       "      <td>1642.447833</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>4.104459</td>\n",
       "      <td>26.924702</td>\n",
       "      <td>72.765042</td>\n",
       "      <td>17.377649</td>\n",
       "      <td>14.775060</td>\n",
       "      <td>61861.0</td>\n",
       "      <td>3082.1</td>\n",
       "      <td>5.531612</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60610</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>93795.0</td>\n",
       "      <td>1537.345214</td>\n",
       "      <td>220.706625</td>\n",
       "      <td>182.978152</td>\n",
       "      <td>498.0</td>\n",
       "      <td>3.149999</td>\n",
       "      <td>11.993181</td>\n",
       "      <td>97.158825</td>\n",
       "      <td>78.236189</td>\n",
       "      <td>69.706756</td>\n",
       "      <td>42422.0</td>\n",
       "      <td>14725.7</td>\n",
       "      <td>1.939278</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60611</td>\n",
       "      <td>117344.0</td>\n",
       "      <td>125624.0</td>\n",
       "      <td>2285.077683</td>\n",
       "      <td>263.585238</td>\n",
       "      <td>86.933274</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1.073433</td>\n",
       "      <td>8.912987</td>\n",
       "      <td>99.282669</td>\n",
       "      <td>87.964404</td>\n",
       "      <td>67.281986</td>\n",
       "      <td>34562.0</td>\n",
       "      <td>16829.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60612</td>\n",
       "      <td>52126.0</td>\n",
       "      <td>37182.0</td>\n",
       "      <td>2377.324853</td>\n",
       "      <td>729.616162</td>\n",
       "      <td>1503.783816</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>5.522375</td>\n",
       "      <td>29.057230</td>\n",
       "      <td>88.019560</td>\n",
       "      <td>39.508383</td>\n",
       "      <td>22.717353</td>\n",
       "      <td>33908.0</td>\n",
       "      <td>3520.5</td>\n",
       "      <td>4.150453</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60613</td>\n",
       "      <td>88267.0</td>\n",
       "      <td>69253.0</td>\n",
       "      <td>1274.959039</td>\n",
       "      <td>194.213557</td>\n",
       "      <td>191.345234</td>\n",
       "      <td>640.0</td>\n",
       "      <td>2.739209</td>\n",
       "      <td>9.531545</td>\n",
       "      <td>96.219923</td>\n",
       "      <td>75.721975</td>\n",
       "      <td>70.781878</td>\n",
       "      <td>53282.0</td>\n",
       "      <td>8836.5</td>\n",
       "      <td>1.973954</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60614</td>\n",
       "      <td>123610.0</td>\n",
       "      <td>97909.0</td>\n",
       "      <td>2384.286946</td>\n",
       "      <td>237.178996</td>\n",
       "      <td>250.078906</td>\n",
       "      <td>571.0</td>\n",
       "      <td>1.696833</td>\n",
       "      <td>8.396480</td>\n",
       "      <td>97.839392</td>\n",
       "      <td>85.894453</td>\n",
       "      <td>79.870769</td>\n",
       "      <td>72119.0</td>\n",
       "      <td>8641.9</td>\n",
       "      <td>3.882247</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>60615</td>\n",
       "      <td>51179.0</td>\n",
       "      <td>43402.0</td>\n",
       "      <td>1730.093521</td>\n",
       "      <td>429.690778</td>\n",
       "      <td>858.868254</td>\n",
       "      <td>680.0</td>\n",
       "      <td>4.657797</td>\n",
       "      <td>25.752881</td>\n",
       "      <td>92.943745</td>\n",
       "      <td>62.095718</td>\n",
       "      <td>28.263940</td>\n",
       "      <td>41767.0</td>\n",
       "      <td>7367.8</td>\n",
       "      <td>3.261086</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>60616</td>\n",
       "      <td>65489.0</td>\n",
       "      <td>43230.0</td>\n",
       "      <td>1935.323290</td>\n",
       "      <td>424.596969</td>\n",
       "      <td>515.734241</td>\n",
       "      <td>641.0</td>\n",
       "      <td>4.394049</td>\n",
       "      <td>17.775055</td>\n",
       "      <td>84.133003</td>\n",
       "      <td>49.902776</td>\n",
       "      <td>29.352925</td>\n",
       "      <td>53085.0</td>\n",
       "      <td>5393.6</td>\n",
       "      <td>2.773658</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>60618</td>\n",
       "      <td>91650.0</td>\n",
       "      <td>51582.0</td>\n",
       "      <td>2050.676663</td>\n",
       "      <td>356.620987</td>\n",
       "      <td>599.281221</td>\n",
       "      <td>928.0</td>\n",
       "      <td>2.832843</td>\n",
       "      <td>8.128349</td>\n",
       "      <td>88.028055</td>\n",
       "      <td>53.002195</td>\n",
       "      <td>48.287722</td>\n",
       "      <td>95078.0</td>\n",
       "      <td>7350.8</td>\n",
       "      <td>4.365800</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60619</td>\n",
       "      <td>43329.0</td>\n",
       "      <td>27002.0</td>\n",
       "      <td>3851.031664</td>\n",
       "      <td>1257.206878</td>\n",
       "      <td>2777.408498</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>5.554942</td>\n",
       "      <td>23.658698</td>\n",
       "      <td>89.481491</td>\n",
       "      <td>27.101045</td>\n",
       "      <td>0.956192</td>\n",
       "      <td>63481.0</td>\n",
       "      <td>4002.8</td>\n",
       "      <td>5.100937</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60620</td>\n",
       "      <td>40117.0</td>\n",
       "      <td>22465.0</td>\n",
       "      <td>4050.777903</td>\n",
       "      <td>1257.469477</td>\n",
       "      <td>2870.865227</td>\n",
       "      <td>1580.0</td>\n",
       "      <td>6.514276</td>\n",
       "      <td>23.263323</td>\n",
       "      <td>87.145217</td>\n",
       "      <td>17.776157</td>\n",
       "      <td>1.115965</td>\n",
       "      <td>69357.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>7.450095</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>60621</td>\n",
       "      <td>24912.0</td>\n",
       "      <td>15738.0</td>\n",
       "      <td>2319.940339</td>\n",
       "      <td>961.451591</td>\n",
       "      <td>2204.373299</td>\n",
       "      <td>1307.0</td>\n",
       "      <td>7.813996</td>\n",
       "      <td>39.933943</td>\n",
       "      <td>81.329277</td>\n",
       "      <td>12.569592</td>\n",
       "      <td>1.499736</td>\n",
       "      <td>26538.0</td>\n",
       "      <td>2729.3</td>\n",
       "      <td>4.282657</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60622</td>\n",
       "      <td>110694.0</td>\n",
       "      <td>72130.0</td>\n",
       "      <td>2244.064291</td>\n",
       "      <td>396.182133</td>\n",
       "      <td>706.355113</td>\n",
       "      <td>983.0</td>\n",
       "      <td>2.325282</td>\n",
       "      <td>9.551578</td>\n",
       "      <td>94.737207</td>\n",
       "      <td>71.883295</td>\n",
       "      <td>64.161025</td>\n",
       "      <td>54650.0</td>\n",
       "      <td>8440.6</td>\n",
       "      <td>2.622911</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60623</td>\n",
       "      <td>35890.0</td>\n",
       "      <td>18124.0</td>\n",
       "      <td>3273.110756</td>\n",
       "      <td>1325.607588</td>\n",
       "      <td>2394.643102</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>4.722798</td>\n",
       "      <td>27.267422</td>\n",
       "      <td>67.917364</td>\n",
       "      <td>12.758747</td>\n",
       "      <td>3.880960</td>\n",
       "      <td>77352.0</td>\n",
       "      <td>5554.6</td>\n",
       "      <td>6.169239</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>60624</td>\n",
       "      <td>29102.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>3065.118531</td>\n",
       "      <td>1376.367880</td>\n",
       "      <td>2715.461936</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>8.900194</td>\n",
       "      <td>36.466104</td>\n",
       "      <td>76.623209</td>\n",
       "      <td>11.139796</td>\n",
       "      <td>3.412102</td>\n",
       "      <td>36986.0</td>\n",
       "      <td>4025.9</td>\n",
       "      <td>5.501184</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>60625</td>\n",
       "      <td>76088.0</td>\n",
       "      <td>40958.0</td>\n",
       "      <td>1523.048410</td>\n",
       "      <td>317.759883</td>\n",
       "      <td>424.924314</td>\n",
       "      <td>842.0</td>\n",
       "      <td>3.351447</td>\n",
       "      <td>12.089427</td>\n",
       "      <td>85.340837</td>\n",
       "      <td>52.082895</td>\n",
       "      <td>46.437497</td>\n",
       "      <td>80028.0</td>\n",
       "      <td>8210.2</td>\n",
       "      <td>3.878165</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>60626</td>\n",
       "      <td>51562.0</td>\n",
       "      <td>33450.0</td>\n",
       "      <td>1500.788632</td>\n",
       "      <td>294.425468</td>\n",
       "      <td>443.967235</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>4.947345</td>\n",
       "      <td>20.059091</td>\n",
       "      <td>89.053526</td>\n",
       "      <td>50.109120</td>\n",
       "      <td>45.787852</td>\n",
       "      <td>50295.0</td>\n",
       "      <td>11313.5</td>\n",
       "      <td>3.025089</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>60628</td>\n",
       "      <td>46665.0</td>\n",
       "      <td>23708.0</td>\n",
       "      <td>3449.929368</td>\n",
       "      <td>1146.086363</td>\n",
       "      <td>2932.147083</td>\n",
       "      <td>1561.0</td>\n",
       "      <td>6.236666</td>\n",
       "      <td>21.966232</td>\n",
       "      <td>86.496334</td>\n",
       "      <td>21.877235</td>\n",
       "      <td>2.267053</td>\n",
       "      <td>64489.0</td>\n",
       "      <td>2268.2</td>\n",
       "      <td>7.862385</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>60629</td>\n",
       "      <td>46217.0</td>\n",
       "      <td>20803.0</td>\n",
       "      <td>2901.001024</td>\n",
       "      <td>849.518997</td>\n",
       "      <td>1761.178058</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>4.063686</td>\n",
       "      <td>18.430556</td>\n",
       "      <td>74.508176</td>\n",
       "      <td>12.145003</td>\n",
       "      <td>7.117140</td>\n",
       "      <td>107487.0</td>\n",
       "      <td>5888.0</td>\n",
       "      <td>4.776712</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>60630</td>\n",
       "      <td>84220.0</td>\n",
       "      <td>40999.0</td>\n",
       "      <td>918.614039</td>\n",
       "      <td>136.394651</td>\n",
       "      <td>159.018281</td>\n",
       "      <td>522.0</td>\n",
       "      <td>3.497598</td>\n",
       "      <td>7.261482</td>\n",
       "      <td>91.223561</td>\n",
       "      <td>43.723071</td>\n",
       "      <td>56.349049</td>\n",
       "      <td>55591.0</td>\n",
       "      <td>4526.9</td>\n",
       "      <td>5.812851</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>60631</td>\n",
       "      <td>103588.0</td>\n",
       "      <td>51385.0</td>\n",
       "      <td>236.405837</td>\n",
       "      <td>26.291313</td>\n",
       "      <td>14.521392</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2.590296</td>\n",
       "      <td>6.486594</td>\n",
       "      <td>94.406895</td>\n",
       "      <td>48.555228</td>\n",
       "      <td>76.367322</td>\n",
       "      <td>30589.0</td>\n",
       "      <td>3152.6</td>\n",
       "      <td>4.750442</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>60632</td>\n",
       "      <td>48791.0</td>\n",
       "      <td>20041.0</td>\n",
       "      <td>1850.434141</td>\n",
       "      <td>492.661063</td>\n",
       "      <td>974.120516</td>\n",
       "      <td>790.0</td>\n",
       "      <td>2.857077</td>\n",
       "      <td>16.250806</td>\n",
       "      <td>70.117388</td>\n",
       "      <td>12.420434</td>\n",
       "      <td>9.057726</td>\n",
       "      <td>87395.0</td>\n",
       "      <td>4529.7</td>\n",
       "      <td>5.432440</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>60633</td>\n",
       "      <td>54074.0</td>\n",
       "      <td>27136.0</td>\n",
       "      <td>192.982686</td>\n",
       "      <td>43.224521</td>\n",
       "      <td>47.273562</td>\n",
       "      <td>158.0</td>\n",
       "      <td>2.506649</td>\n",
       "      <td>14.718720</td>\n",
       "      <td>85.621139</td>\n",
       "      <td>14.630519</td>\n",
       "      <td>31.868392</td>\n",
       "      <td>12674.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>5.436372</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60634</td>\n",
       "      <td>75878.0</td>\n",
       "      <td>33915.0</td>\n",
       "      <td>1045.357086</td>\n",
       "      <td>174.284279</td>\n",
       "      <td>287.366740</td>\n",
       "      <td>815.0</td>\n",
       "      <td>2.971386</td>\n",
       "      <td>9.313472</td>\n",
       "      <td>87.322395</td>\n",
       "      <td>24.899026</td>\n",
       "      <td>49.975490</td>\n",
       "      <td>77520.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>5.108346</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60636</td>\n",
       "      <td>27831.0</td>\n",
       "      <td>18491.0</td>\n",
       "      <td>2383.669839</td>\n",
       "      <td>948.662395</td>\n",
       "      <td>2229.141811</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>7.245328</td>\n",
       "      <td>31.380560</td>\n",
       "      <td>78.897420</td>\n",
       "      <td>10.081549</td>\n",
       "      <td>1.121268</td>\n",
       "      <td>30412.0</td>\n",
       "      <td>3001.1</td>\n",
       "      <td>6.049819</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>60637</td>\n",
       "      <td>31720.0</td>\n",
       "      <td>29713.0</td>\n",
       "      <td>2751.077827</td>\n",
       "      <td>935.194200</td>\n",
       "      <td>2108.576860</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>5.471867</td>\n",
       "      <td>33.773802</td>\n",
       "      <td>87.797311</td>\n",
       "      <td>34.756647</td>\n",
       "      <td>14.892758</td>\n",
       "      <td>49514.0</td>\n",
       "      <td>4222.4</td>\n",
       "      <td>2.771199</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>60638</td>\n",
       "      <td>76258.0</td>\n",
       "      <td>33003.0</td>\n",
       "      <td>818.493636</td>\n",
       "      <td>126.229517</td>\n",
       "      <td>146.290743</td>\n",
       "      <td>569.0</td>\n",
       "      <td>2.369443</td>\n",
       "      <td>6.905349</td>\n",
       "      <td>85.064553</td>\n",
       "      <td>23.170193</td>\n",
       "      <td>40.699469</td>\n",
       "      <td>58959.0</td>\n",
       "      <td>2057.4</td>\n",
       "      <td>5.689823</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>60639</td>\n",
       "      <td>53197.0</td>\n",
       "      <td>22417.0</td>\n",
       "      <td>2063.226585</td>\n",
       "      <td>544.187096</td>\n",
       "      <td>1254.774483</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>5.060198</td>\n",
       "      <td>17.387277</td>\n",
       "      <td>69.799299</td>\n",
       "      <td>15.010088</td>\n",
       "      <td>9.014230</td>\n",
       "      <td>89037.0</td>\n",
       "      <td>6927.5</td>\n",
       "      <td>5.311878</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>60640</td>\n",
       "      <td>63660.0</td>\n",
       "      <td>48583.0</td>\n",
       "      <td>1550.604239</td>\n",
       "      <td>264.665437</td>\n",
       "      <td>381.435688</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>5.010432</td>\n",
       "      <td>19.000540</td>\n",
       "      <td>92.044185</td>\n",
       "      <td>61.085567</td>\n",
       "      <td>55.453662</td>\n",
       "      <td>67936.0</td>\n",
       "      <td>11108.3</td>\n",
       "      <td>2.328343</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>60641</td>\n",
       "      <td>72950.0</td>\n",
       "      <td>34565.0</td>\n",
       "      <td>1404.401567</td>\n",
       "      <td>287.848534</td>\n",
       "      <td>459.195616</td>\n",
       "      <td>901.0</td>\n",
       "      <td>2.984210</td>\n",
       "      <td>11.506682</td>\n",
       "      <td>82.093574</td>\n",
       "      <td>32.203805</td>\n",
       "      <td>35.409771</td>\n",
       "      <td>71003.0</td>\n",
       "      <td>6808.1</td>\n",
       "      <td>4.688010</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>60642</td>\n",
       "      <td>131201.0</td>\n",
       "      <td>77312.0</td>\n",
       "      <td>1076.700303</td>\n",
       "      <td>131.883872</td>\n",
       "      <td>214.026508</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1.480857</td>\n",
       "      <td>8.116078</td>\n",
       "      <td>95.846610</td>\n",
       "      <td>75.946112</td>\n",
       "      <td>63.371386</td>\n",
       "      <td>21374.0</td>\n",
       "      <td>4948.5</td>\n",
       "      <td>1.525203</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>60643</td>\n",
       "      <td>73285.0</td>\n",
       "      <td>37873.0</td>\n",
       "      <td>1705.193726</td>\n",
       "      <td>413.179874</td>\n",
       "      <td>1066.324539</td>\n",
       "      <td>762.0</td>\n",
       "      <td>4.566340</td>\n",
       "      <td>14.416256</td>\n",
       "      <td>92.687617</td>\n",
       "      <td>36.912039</td>\n",
       "      <td>21.200498</td>\n",
       "      <td>49796.0</td>\n",
       "      <td>2613.9</td>\n",
       "      <td>6.153455</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>60644</td>\n",
       "      <td>31816.0</td>\n",
       "      <td>18426.0</td>\n",
       "      <td>2990.778663</td>\n",
       "      <td>1222.738852</td>\n",
       "      <td>2600.019557</td>\n",
       "      <td>2278.0</td>\n",
       "      <td>6.977786</td>\n",
       "      <td>29.677682</td>\n",
       "      <td>84.323003</td>\n",
       "      <td>11.663280</td>\n",
       "      <td>3.802225</td>\n",
       "      <td>47630.0</td>\n",
       "      <td>5238.1</td>\n",
       "      <td>4.045238</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>60645</td>\n",
       "      <td>63488.0</td>\n",
       "      <td>31554.0</td>\n",
       "      <td>834.021725</td>\n",
       "      <td>147.062184</td>\n",
       "      <td>230.767219</td>\n",
       "      <td>653.0</td>\n",
       "      <td>3.043903</td>\n",
       "      <td>16.903430</td>\n",
       "      <td>86.762725</td>\n",
       "      <td>45.779170</td>\n",
       "      <td>44.328496</td>\n",
       "      <td>48585.0</td>\n",
       "      <td>8427.5</td>\n",
       "      <td>4.464840</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>60646</td>\n",
       "      <td>103913.0</td>\n",
       "      <td>57524.0</td>\n",
       "      <td>323.162073</td>\n",
       "      <td>35.822947</td>\n",
       "      <td>39.111622</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3.112240</td>\n",
       "      <td>6.523007</td>\n",
       "      <td>93.636931</td>\n",
       "      <td>54.076491</td>\n",
       "      <td>71.250767</td>\n",
       "      <td>29326.0</td>\n",
       "      <td>2551.5</td>\n",
       "      <td>7.805828</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>60647</td>\n",
       "      <td>89974.0</td>\n",
       "      <td>55599.0</td>\n",
       "      <td>2435.283370</td>\n",
       "      <td>403.003988</td>\n",
       "      <td>812.883397</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>2.863707</td>\n",
       "      <td>11.700303</td>\n",
       "      <td>90.880955</td>\n",
       "      <td>59.019007</td>\n",
       "      <td>49.169043</td>\n",
       "      <td>85564.0</td>\n",
       "      <td>8336.7</td>\n",
       "      <td>6.989061</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>60649</td>\n",
       "      <td>38026.0</td>\n",
       "      <td>27840.0</td>\n",
       "      <td>2864.170064</td>\n",
       "      <td>972.598647</td>\n",
       "      <td>1780.167049</td>\n",
       "      <td>1367.0</td>\n",
       "      <td>5.583872</td>\n",
       "      <td>30.505475</td>\n",
       "      <td>89.525163</td>\n",
       "      <td>25.447825</td>\n",
       "      <td>2.846466</td>\n",
       "      <td>48973.0</td>\n",
       "      <td>5223.5</td>\n",
       "      <td>5.403068</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>60652</td>\n",
       "      <td>70766.0</td>\n",
       "      <td>26389.0</td>\n",
       "      <td>790.830432</td>\n",
       "      <td>188.898692</td>\n",
       "      <td>482.852542</td>\n",
       "      <td>417.0</td>\n",
       "      <td>3.776854</td>\n",
       "      <td>13.712251</td>\n",
       "      <td>80.484580</td>\n",
       "      <td>19.090162</td>\n",
       "      <td>8.313211</td>\n",
       "      <td>43485.0</td>\n",
       "      <td>3470.4</td>\n",
       "      <td>4.833535</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>60653</td>\n",
       "      <td>34564.0</td>\n",
       "      <td>33105.0</td>\n",
       "      <td>1503.001240</td>\n",
       "      <td>422.510839</td>\n",
       "      <td>919.043312</td>\n",
       "      <td>893.0</td>\n",
       "      <td>7.083321</td>\n",
       "      <td>29.963846</td>\n",
       "      <td>89.587519</td>\n",
       "      <td>37.965377</td>\n",
       "      <td>6.292187</td>\n",
       "      <td>34519.0</td>\n",
       "      <td>5648.5</td>\n",
       "      <td>4.815664</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>60654</td>\n",
       "      <td>127072.0</td>\n",
       "      <td>125409.0</td>\n",
       "      <td>1610.559349</td>\n",
       "      <td>232.338620</td>\n",
       "      <td>140.931338</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.301379</td>\n",
       "      <td>6.743218</td>\n",
       "      <td>98.928935</td>\n",
       "      <td>88.925186</td>\n",
       "      <td>71.023377</td>\n",
       "      <td>21859.0</td>\n",
       "      <td>15577.5</td>\n",
       "      <td>0.991884</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>60655</td>\n",
       "      <td>105229.0</td>\n",
       "      <td>45696.0</td>\n",
       "      <td>297.724496</td>\n",
       "      <td>40.294259</td>\n",
       "      <td>55.273134</td>\n",
       "      <td>274.0</td>\n",
       "      <td>2.413829</td>\n",
       "      <td>4.328342</td>\n",
       "      <td>95.935213</td>\n",
       "      <td>45.247365</td>\n",
       "      <td>81.058050</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>2518.3</td>\n",
       "      <td>4.225563</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>60656</td>\n",
       "      <td>75006.0</td>\n",
       "      <td>42361.0</td>\n",
       "      <td>308.143633</td>\n",
       "      <td>35.884060</td>\n",
       "      <td>51.104736</td>\n",
       "      <td>183.0</td>\n",
       "      <td>3.016006</td>\n",
       "      <td>9.352869</td>\n",
       "      <td>91.992767</td>\n",
       "      <td>40.768731</td>\n",
       "      <td>69.865690</td>\n",
       "      <td>28665.0</td>\n",
       "      <td>3419.3</td>\n",
       "      <td>4.239275</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>60657</td>\n",
       "      <td>98457.0</td>\n",
       "      <td>83102.0</td>\n",
       "      <td>2081.738781</td>\n",
       "      <td>256.490141</td>\n",
       "      <td>252.368275</td>\n",
       "      <td>560.0</td>\n",
       "      <td>1.989414</td>\n",
       "      <td>7.504769</td>\n",
       "      <td>98.591288</td>\n",
       "      <td>85.360519</td>\n",
       "      <td>76.801686</td>\n",
       "      <td>72113.0</td>\n",
       "      <td>12804.2</td>\n",
       "      <td>2.156030</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>60659</td>\n",
       "      <td>57077.0</td>\n",
       "      <td>26819.0</td>\n",
       "      <td>810.169875</td>\n",
       "      <td>146.761300</td>\n",
       "      <td>172.368192</td>\n",
       "      <td>344.0</td>\n",
       "      <td>3.886446</td>\n",
       "      <td>17.832957</td>\n",
       "      <td>80.002104</td>\n",
       "      <td>37.961632</td>\n",
       "      <td>35.939537</td>\n",
       "      <td>43729.0</td>\n",
       "      <td>7155.2</td>\n",
       "      <td>4.690863</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>60660</td>\n",
       "      <td>56090.0</td>\n",
       "      <td>41774.0</td>\n",
       "      <td>827.359735</td>\n",
       "      <td>150.962529</td>\n",
       "      <td>155.643638</td>\n",
       "      <td>693.0</td>\n",
       "      <td>5.707603</td>\n",
       "      <td>16.390555</td>\n",
       "      <td>90.766556</td>\n",
       "      <td>53.799802</td>\n",
       "      <td>49.788757</td>\n",
       "      <td>44972.0</td>\n",
       "      <td>13631.6</td>\n",
       "      <td>3.397911</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>60707</td>\n",
       "      <td>68430.0</td>\n",
       "      <td>34022.0</td>\n",
       "      <td>359.436253</td>\n",
       "      <td>55.548957</td>\n",
       "      <td>90.783226</td>\n",
       "      <td>462.0</td>\n",
       "      <td>3.686221</td>\n",
       "      <td>7.501034</td>\n",
       "      <td>86.743672</td>\n",
       "      <td>29.947342</td>\n",
       "      <td>46.975235</td>\n",
       "      <td>41309.0</td>\n",
       "      <td>4457.8</td>\n",
       "      <td>5.224502</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>60827</td>\n",
       "      <td>32801.0</td>\n",
       "      <td>19032.0</td>\n",
       "      <td>399.762520</td>\n",
       "      <td>158.076267</td>\n",
       "      <td>334.661348</td>\n",
       "      <td>445.0</td>\n",
       "      <td>3.765385</td>\n",
       "      <td>32.092718</td>\n",
       "      <td>89.206831</td>\n",
       "      <td>14.597481</td>\n",
       "      <td>1.599385</td>\n",
       "      <td>26010.0</td>\n",
       "      <td>1429.1</td>\n",
       "      <td>8.549290</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zip  median_INC_2017-2021  per_capita_INC_2017-2021  \\\n",
       "0   60601              123115.0                  137556.0   \n",
       "1   60602              235666.0                  168423.0   \n",
       "2   60605              112162.0                   85947.0   \n",
       "3   60606              130196.0                  133605.0   \n",
       "4   60607              109634.0                   65052.0   \n",
       "5   60608               61490.0                   26958.0   \n",
       "6   60609               43048.0                   23009.0   \n",
       "7   60610               99246.0                   93795.0   \n",
       "8   60611              117344.0                  125624.0   \n",
       "9   60612               52126.0                   37182.0   \n",
       "10  60613               88267.0                   69253.0   \n",
       "11  60614              123610.0                   97909.0   \n",
       "12  60615               51179.0                   43402.0   \n",
       "13  60616               65489.0                   43230.0   \n",
       "14  60618               91650.0                   51582.0   \n",
       "15  60619               43329.0                   27002.0   \n",
       "16  60620               40117.0                   22465.0   \n",
       "17  60621               24912.0                   15738.0   \n",
       "18  60622              110694.0                   72130.0   \n",
       "19  60623               35890.0                   18124.0   \n",
       "20  60624               29102.0                   17145.0   \n",
       "21  60625               76088.0                   40958.0   \n",
       "22  60626               51562.0                   33450.0   \n",
       "23  60628               46665.0                   23708.0   \n",
       "24  60629               46217.0                   20803.0   \n",
       "25  60630               84220.0                   40999.0   \n",
       "26  60631              103588.0                   51385.0   \n",
       "27  60632               48791.0                   20041.0   \n",
       "28  60633               54074.0                   27136.0   \n",
       "29  60634               75878.0                   33915.0   \n",
       "30  60636               27831.0                   18491.0   \n",
       "31  60637               31720.0                   29713.0   \n",
       "32  60638               76258.0                   33003.0   \n",
       "33  60639               53197.0                   22417.0   \n",
       "34  60640               63660.0                   48583.0   \n",
       "35  60641               72950.0                   34565.0   \n",
       "36  60642              131201.0                   77312.0   \n",
       "37  60643               73285.0                   37873.0   \n",
       "38  60644               31816.0                   18426.0   \n",
       "39  60645               63488.0                   31554.0   \n",
       "40  60646              103913.0                   57524.0   \n",
       "41  60647               89974.0                   55599.0   \n",
       "42  60649               38026.0                   27840.0   \n",
       "43  60652               70766.0                   26389.0   \n",
       "44  60653               34564.0                   33105.0   \n",
       "45  60654              127072.0                  125409.0   \n",
       "46  60655              105229.0                   45696.0   \n",
       "47  60656               75006.0                   42361.0   \n",
       "48  60657               98457.0                   83102.0   \n",
       "49  60659               57077.0                   26819.0   \n",
       "50  60660               56090.0                   41774.0   \n",
       "51  60707               68430.0                   34022.0   \n",
       "52  60827               32801.0                   19032.0   \n",
       "\n",
       "    major_crime_2018-2022  violent_crime_2018-2022  public_crime_2012-2016  \\\n",
       "0             1736.752366               148.889510               57.189159   \n",
       "1              282.033895                25.289746                5.774626   \n",
       "2             1675.218975               246.799483              126.443697   \n",
       "3              529.774357                58.453847               20.751522   \n",
       "4             2208.933790               266.924943              266.380039   \n",
       "5             2208.318895               582.919388              978.662767   \n",
       "6             2179.855381               760.609450             1642.447833   \n",
       "7             1537.345214               220.706625              182.978152   \n",
       "8             2285.077683               263.585238               86.933274   \n",
       "9             2377.324853               729.616162             1503.783816   \n",
       "10            1274.959039               194.213557              191.345234   \n",
       "11            2384.286946               237.178996              250.078906   \n",
       "12            1730.093521               429.690778              858.868254   \n",
       "13            1935.323290               424.596969              515.734241   \n",
       "14            2050.676663               356.620987              599.281221   \n",
       "15            3851.031664              1257.206878             2777.408498   \n",
       "16            4050.777903              1257.469477             2870.865227   \n",
       "17            2319.940339               961.451591             2204.373299   \n",
       "18            2244.064291               396.182133              706.355113   \n",
       "19            3273.110756              1325.607588             2394.643102   \n",
       "20            3065.118531              1376.367880             2715.461936   \n",
       "21            1523.048410               317.759883              424.924314   \n",
       "22            1500.788632               294.425468              443.967235   \n",
       "23            3449.929368              1146.086363             2932.147083   \n",
       "24            2901.001024               849.518997             1761.178058   \n",
       "25             918.614039               136.394651              159.018281   \n",
       "26             236.405837                26.291313               14.521392   \n",
       "27            1850.434141               492.661063              974.120516   \n",
       "28             192.982686                43.224521               47.273562   \n",
       "29            1045.357086               174.284279              287.366740   \n",
       "30            2383.669839               948.662395             2229.141811   \n",
       "31            2751.077827               935.194200             2108.576860   \n",
       "32             818.493636               126.229517              146.290743   \n",
       "33            2063.226585               544.187096             1254.774483   \n",
       "34            1550.604239               264.665437              381.435688   \n",
       "35            1404.401567               287.848534              459.195616   \n",
       "36            1076.700303               131.883872              214.026508   \n",
       "37            1705.193726               413.179874             1066.324539   \n",
       "38            2990.778663              1222.738852             2600.019557   \n",
       "39             834.021725               147.062184              230.767219   \n",
       "40             323.162073                35.822947               39.111622   \n",
       "41            2435.283370               403.003988              812.883397   \n",
       "42            2864.170064               972.598647             1780.167049   \n",
       "43             790.830432               188.898692              482.852542   \n",
       "44            1503.001240               422.510839              919.043312   \n",
       "45            1610.559349               232.338620              140.931338   \n",
       "46             297.724496                40.294259               55.273134   \n",
       "47             308.143633                35.884060               51.104736   \n",
       "48            2081.738781               256.490141              252.368275   \n",
       "49             810.169875               146.761300              172.368192   \n",
       "50             827.359735               150.962529              155.643638   \n",
       "51             359.436253                55.548957               90.783226   \n",
       "52             399.762520               158.076267              334.661348   \n",
       "\n",
       "    behavioral_ health_hospitalizations_2017  \\\n",
       "0                                       91.0   \n",
       "1                                       25.0   \n",
       "2                                      285.0   \n",
       "3                                       66.0   \n",
       "4                                      651.0   \n",
       "5                                     1137.0   \n",
       "6                                     1116.0   \n",
       "7                                      498.0   \n",
       "8                                      248.0   \n",
       "9                                     1366.0   \n",
       "10                                     640.0   \n",
       "11                                     571.0   \n",
       "12                                     680.0   \n",
       "13                                     641.0   \n",
       "14                                     928.0   \n",
       "15                                    1421.0   \n",
       "16                                    1580.0   \n",
       "17                                    1307.0   \n",
       "18                                     983.0   \n",
       "19                                    1870.0   \n",
       "20                                    1510.0   \n",
       "21                                     842.0   \n",
       "22                                    1525.0   \n",
       "23                                    1561.0   \n",
       "24                                    1242.0   \n",
       "25                                     522.0   \n",
       "26                                     254.0   \n",
       "27                                     790.0   \n",
       "28                                     158.0   \n",
       "29                                     815.0   \n",
       "30                                    1303.0   \n",
       "31                                    1291.0   \n",
       "32                                     569.0   \n",
       "33                                    1235.0   \n",
       "34                                    2139.0   \n",
       "35                                     901.0   \n",
       "36                                     197.0   \n",
       "37                                     762.0   \n",
       "38                                    2278.0   \n",
       "39                                     653.0   \n",
       "40                                     181.0   \n",
       "41                                    1253.0   \n",
       "42                                    1367.0   \n",
       "43                                     417.0   \n",
       "44                                     893.0   \n",
       "45                                     181.0   \n",
       "46                                     274.0   \n",
       "47                                     183.0   \n",
       "48                                     560.0   \n",
       "49                                     344.0   \n",
       "50                                     693.0   \n",
       "51                                     462.0   \n",
       "52                                     445.0   \n",
       "\n",
       "    cognitive_difficulty_percent_2017-2021  poverty_rate_percent_2017-2021  \\\n",
       "0                                 1.995805                        5.834093   \n",
       "1                                 0.000000                        2.222222   \n",
       "2                                 2.914663                        8.612424   \n",
       "3                                 0.363857                        8.550637   \n",
       "4                                 4.155217                       16.888971   \n",
       "5                                 3.625513                       16.787975   \n",
       "6                                 4.104459                       26.924702   \n",
       "7                                 3.149999                       11.993181   \n",
       "8                                 1.073433                        8.912987   \n",
       "9                                 5.522375                       29.057230   \n",
       "10                                2.739209                        9.531545   \n",
       "11                                1.696833                        8.396480   \n",
       "12                                4.657797                       25.752881   \n",
       "13                                4.394049                       17.775055   \n",
       "14                                2.832843                        8.128349   \n",
       "15                                5.554942                       23.658698   \n",
       "16                                6.514276                       23.263323   \n",
       "17                                7.813996                       39.933943   \n",
       "18                                2.325282                        9.551578   \n",
       "19                                4.722798                       27.267422   \n",
       "20                                8.900194                       36.466104   \n",
       "21                                3.351447                       12.089427   \n",
       "22                                4.947345                       20.059091   \n",
       "23                                6.236666                       21.966232   \n",
       "24                                4.063686                       18.430556   \n",
       "25                                3.497598                        7.261482   \n",
       "26                                2.590296                        6.486594   \n",
       "27                                2.857077                       16.250806   \n",
       "28                                2.506649                       14.718720   \n",
       "29                                2.971386                        9.313472   \n",
       "30                                7.245328                       31.380560   \n",
       "31                                5.471867                       33.773802   \n",
       "32                                2.369443                        6.905349   \n",
       "33                                5.060198                       17.387277   \n",
       "34                                5.010432                       19.000540   \n",
       "35                                2.984210                       11.506682   \n",
       "36                                1.480857                        8.116078   \n",
       "37                                4.566340                       14.416256   \n",
       "38                                6.977786                       29.677682   \n",
       "39                                3.043903                       16.903430   \n",
       "40                                3.112240                        6.523007   \n",
       "41                                2.863707                       11.700303   \n",
       "42                                5.583872                       30.505475   \n",
       "43                                3.776854                       13.712251   \n",
       "44                                7.083321                       29.963846   \n",
       "45                                1.301379                        6.743218   \n",
       "46                                2.413829                        4.328342   \n",
       "47                                3.016006                        9.352869   \n",
       "48                                1.989414                        7.504769   \n",
       "49                                3.886446                       17.832957   \n",
       "50                                5.707603                       16.390555   \n",
       "51                                3.686221                        7.501034   \n",
       "52                                3.765385                       32.092718   \n",
       "\n",
       "    high_school_grad_rate_2017-2021  college_grad_rate_2017-2021  \\\n",
       "0                         97.917725                    88.031149   \n",
       "1                        100.000000                    82.217782   \n",
       "2                         95.316371                    78.088010   \n",
       "3                         99.266177                    91.794530   \n",
       "4                         96.620012                    78.920985   \n",
       "5                         74.265756                    30.445646   \n",
       "6                         72.765042                    17.377649   \n",
       "7                         97.158825                    78.236189   \n",
       "8                         99.282669                    87.964404   \n",
       "9                         88.019560                    39.508383   \n",
       "10                        96.219923                    75.721975   \n",
       "11                        97.839392                    85.894453   \n",
       "12                        92.943745                    62.095718   \n",
       "13                        84.133003                    49.902776   \n",
       "14                        88.028055                    53.002195   \n",
       "15                        89.481491                    27.101045   \n",
       "16                        87.145217                    17.776157   \n",
       "17                        81.329277                    12.569592   \n",
       "18                        94.737207                    71.883295   \n",
       "19                        67.917364                    12.758747   \n",
       "20                        76.623209                    11.139796   \n",
       "21                        85.340837                    52.082895   \n",
       "22                        89.053526                    50.109120   \n",
       "23                        86.496334                    21.877235   \n",
       "24                        74.508176                    12.145003   \n",
       "25                        91.223561                    43.723071   \n",
       "26                        94.406895                    48.555228   \n",
       "27                        70.117388                    12.420434   \n",
       "28                        85.621139                    14.630519   \n",
       "29                        87.322395                    24.899026   \n",
       "30                        78.897420                    10.081549   \n",
       "31                        87.797311                    34.756647   \n",
       "32                        85.064553                    23.170193   \n",
       "33                        69.799299                    15.010088   \n",
       "34                        92.044185                    61.085567   \n",
       "35                        82.093574                    32.203805   \n",
       "36                        95.846610                    75.946112   \n",
       "37                        92.687617                    36.912039   \n",
       "38                        84.323003                    11.663280   \n",
       "39                        86.762725                    45.779170   \n",
       "40                        93.636931                    54.076491   \n",
       "41                        90.880955                    59.019007   \n",
       "42                        89.525163                    25.447825   \n",
       "43                        80.484580                    19.090162   \n",
       "44                        89.587519                    37.965377   \n",
       "45                        98.928935                    88.925186   \n",
       "46                        95.935213                    45.247365   \n",
       "47                        91.992767                    40.768731   \n",
       "48                        98.591288                    85.360519   \n",
       "49                        80.002104                    37.961632   \n",
       "50                        90.766556                    53.799802   \n",
       "51                        86.743672                    29.947342   \n",
       "52                        89.206831                    14.597481   \n",
       "\n",
       "    non_hispanic_white_percent_2017-2021  population  density  lead_level  \\\n",
       "0                              54.877060     14804.0  14706.9    0.940404   \n",
       "1                              47.022767      1142.0   5350.7    1.464326   \n",
       "2                              57.037753     32077.0   8658.3    0.911358   \n",
       "3                              59.642207      3298.0   5487.1    1.533333   \n",
       "4                              51.797266     29990.0   5005.4    1.676312   \n",
       "5                              20.245194     83689.0   5172.7    3.682467   \n",
       "6                              14.775060     61861.0   3082.1    5.531612   \n",
       "7                              69.706756     42422.0  14725.7    1.939278   \n",
       "8                              67.281986     34562.0  16829.4    1.000000   \n",
       "9                              22.717353     33908.0   3520.5    4.150453   \n",
       "10                             70.781878     53282.0   8836.5    1.973954   \n",
       "11                             79.870769     72119.0   8641.9    3.882247   \n",
       "12                             28.263940     41767.0   7367.8    3.261086   \n",
       "13                             29.352925     53085.0   5393.6    2.773658   \n",
       "14                             48.287722     95078.0   7350.8    4.365800   \n",
       "15                              0.956192     63481.0   4002.8    5.100937   \n",
       "16                              1.115965     69357.0   3827.0    7.450095   \n",
       "17                              1.499736     26538.0   2729.3    4.282657   \n",
       "18                             64.161025     54650.0   8440.6    2.622911   \n",
       "19                              3.880960     77352.0   5554.6    6.169239   \n",
       "20                              3.412102     36986.0   4025.9    5.501184   \n",
       "21                             46.437497     80028.0   8210.2    3.878165   \n",
       "22                             45.787852     50295.0  11313.5    3.025089   \n",
       "23                              2.267053     64489.0   2268.2    7.862385   \n",
       "24                              7.117140    107487.0   5888.0    4.776712   \n",
       "25                             56.349049     55591.0   4526.9    5.812851   \n",
       "26                             76.367322     30589.0   3152.6    4.750442   \n",
       "27                              9.057726     87395.0   4529.7    5.432440   \n",
       "28                             31.868392     12674.0    502.0    5.436372   \n",
       "29                             49.975490     77520.0   4210.0    5.108346   \n",
       "30                              1.121268     30412.0   3001.1    6.049819   \n",
       "31                             14.892758     49514.0   4222.4    2.771199   \n",
       "32                             40.699469     58959.0   2057.4    5.689823   \n",
       "33                              9.014230     89037.0   6927.5    5.311878   \n",
       "34                             55.453662     67936.0  11108.3    2.328343   \n",
       "35                             35.409771     71003.0   6808.1    4.688010   \n",
       "36                             63.371386     21374.0   4948.5    1.525203   \n",
       "37                             21.200498     49796.0   2613.9    6.153455   \n",
       "38                              3.802225     47630.0   5238.1    4.045238   \n",
       "39                             44.328496     48585.0   8427.5    4.464840   \n",
       "40                             71.250767     29326.0   2551.5    7.805828   \n",
       "41                             49.169043     85564.0   8336.7    6.989061   \n",
       "42                              2.846466     48973.0   5223.5    5.403068   \n",
       "43                              8.313211     43485.0   3470.4    4.833535   \n",
       "44                              6.292187     34519.0   5648.5    4.815664   \n",
       "45                             71.023377     21859.0  15577.5    0.991884   \n",
       "46                             81.058050     28751.0   2518.3    4.225563   \n",
       "47                             69.865690     28665.0   3419.3    4.239275   \n",
       "48                             76.801686     72113.0  12804.2    2.156030   \n",
       "49                             35.939537     43729.0   7155.2    4.690863   \n",
       "50                             49.788757     44972.0  13631.6    3.397911   \n",
       "51                             46.975235     41309.0   4457.8    5.224502   \n",
       "52                              1.599385     26010.0   1429.1    8.549290   \n",
       "\n",
       "   lead_level_cat  \n",
       "0             Low  \n",
       "1          Medium  \n",
       "2             Low  \n",
       "3          Medium  \n",
       "4          Medium  \n",
       "5          Medium  \n",
       "6            High  \n",
       "7          Medium  \n",
       "8             Low  \n",
       "9          Medium  \n",
       "10         Medium  \n",
       "11         Medium  \n",
       "12         Medium  \n",
       "13         Medium  \n",
       "14         Medium  \n",
       "15           High  \n",
       "16           High  \n",
       "17         Medium  \n",
       "18         Medium  \n",
       "19           High  \n",
       "20           High  \n",
       "21         Medium  \n",
       "22         Medium  \n",
       "23           High  \n",
       "24         Medium  \n",
       "25           High  \n",
       "26         Medium  \n",
       "27           High  \n",
       "28           High  \n",
       "29           High  \n",
       "30           High  \n",
       "31         Medium  \n",
       "32           High  \n",
       "33           High  \n",
       "34         Medium  \n",
       "35         Medium  \n",
       "36         Medium  \n",
       "37           High  \n",
       "38         Medium  \n",
       "39         Medium  \n",
       "40           High  \n",
       "41           High  \n",
       "42           High  \n",
       "43         Medium  \n",
       "44         Medium  \n",
       "45            Low  \n",
       "46         Medium  \n",
       "47         Medium  \n",
       "48         Medium  \n",
       "49         Medium  \n",
       "50         Medium  \n",
       "51           High  \n",
       "52           High  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('detail_zip.csv',index_col=0)\n",
    "temp_df = pd.read_csv('uszips.csv')\n",
    "temp_df = temp_df[temp_df['state_id'] == 'IL'].reset_index(drop=True)\n",
    "temp_df = temp_df[['zip','population','density']]\n",
    "df = df.merge(temp_df, on='zip', how='left')\n",
    "df['lead_level'] = (df['1st Draw'] + df['2/3 Min'] + df['5 Min']) / 3\n",
    "df = df.drop(columns=['1st Draw', '2/3 Min', '5 Min'])\n",
    "bins = [0, 1, 5, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "df['lead_level_cat'] = pd.cut(df['lead_level'], bins=bins, labels=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36271395",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21057425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 13) (53,)\n",
      "53 53\n",
      "(42, 13) (11, 13)\n",
      "(42,) (11,)\n",
      "42 11\n",
      "42 11\n"
     ]
    }
   ],
   "source": [
    "X = df[['median_INC_2017-2021', 'per_capita_INC_2017-2021',\n",
    "       'major_crime_2018-2022', 'violent_crime_2018-2022',\n",
    "       'public_crime_2012-2016', 'behavioral_ health_hospitalizations_2017',\n",
    "       'cognitive_difficulty_percent_2017-2021',\n",
    "       'poverty_rate_percent_2017-2021', 'high_school_grad_rate_2017-2021',\n",
    "       'college_grad_rate_2017-2021', 'non_hispanic_white_percent_2017-2021',\n",
    "       'population', 'density']].astype(float)\n",
    "y = df['lead_level_cat']\n",
    "\n",
    "y = y.replace('Low', 0).replace('Medium', 1).replace('High', 2)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "x_train = X.iloc[:42,:]\n",
    "x_test = X.iloc[42:,:]\n",
    "\n",
    "y_train = y.iloc[:42]\n",
    "y_test = y.iloc[42:]\n",
    "\n",
    "#print(y_train, y_test)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(len(X), len(y))\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(len(x_train), len(x_test))\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1a5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1():\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(3, activation = 'linear'), \n",
    "                             tf.keras.layers.Dense(3, activation = 'linear'),\n",
    "                             tf.keras.layers.Dense(5, activation = 'linear'),\n",
    "                             tf.keras.layers.Dense(1, activation = 'linear')])\n",
    "\n",
    "def create_model2():\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(3, activation = 'linear'), \n",
    "                             tf.keras.layers.Dense(5, activation = 'relu'),\n",
    "                             tf.keras.layers.Dense(2, activation = 'linear'),\n",
    "                             tf.keras.layers.Dense(1, activation = 'linear')])\n",
    "\n",
    "def create_model3():\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(100, activation = 'relu'), \n",
    "                             tf.keras.layers.Dense(10, activation = 'relu'),\n",
    "                             tf.keras.layers.Dense(1, activation = 'sigmoid')])\n",
    "\n",
    "def train_models():\n",
    "    \n",
    "\n",
    "    model1 = create_model1()\n",
    "    model1.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.SGD(), \n",
    "                metrics = ['accuracy'])\n",
    "    \n",
    "    model2 = create_model2()\n",
    "    model2.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.SGD(), \n",
    "                metrics = ['accuracy'])\n",
    "    \n",
    "    model3 = create_model3()\n",
    "    model3.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.SGD(), \n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    logdir1 = os.path.join(\"logs1\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    logdir2 = os.path.join(\"logs2\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    logdir3 = os.path.join(\"logs3\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboardCallback1 = tf.keras.callbacks.TensorBoard(logdir1, histogram_freq=1)\n",
    "    tensorboardCallback2 = tf.keras.callbacks.TensorBoard(logdir2, histogram_freq=1)\n",
    "    tensorboardCallback3 = tf.keras.callbacks.TensorBoard(logdir3, histogram_freq=1)\n",
    "\n",
    "    model1.fit(X, y, epochs = 100, callbacks = [tensorboardCallback1])\n",
    "    model2.fit(X, y, epochs = 100, callbacks = [tensorboardCallback2])\n",
    "    model3.fit(X, y, epochs = 100, callbacks = [tensorboardCallback3])\n",
    "    \n",
    "    \n",
    "def train_models2():\n",
    "    \n",
    "\n",
    "    model1 = create_model1()\n",
    "    model1.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.SGD(), \n",
    "                metrics = ['accuracy'])\n",
    "    \n",
    "    model2 = create_model2()\n",
    "    model2.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.SGD(), \n",
    "                metrics = ['accuracy'])\n",
    "    \n",
    "    model3 = create_model3()\n",
    "    model3.compile(loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.SGD(), \n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    logdir1 = os.path.join(\"logs4\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    logdir2 = os.path.join(\"logs5\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    logdir3 = os.path.join(\"logs6\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboardCallback1 = tf.keras.callbacks.TensorBoard(logdir1, histogram_freq=1)\n",
    "    tensorboardCallback2 = tf.keras.callbacks.TensorBoard(logdir2, histogram_freq=1)\n",
    "    tensorboardCallback3 = tf.keras.callbacks.TensorBoard(logdir3, histogram_freq=1)\n",
    "\n",
    "    model1.fit(X, y, epochs = 100, validation_data = (x_test, y_test), callbacks = [tensorboardCallback1])\n",
    "    model2.fit(X, y, epochs = 100, validation_data = (x_test, y_test), callbacks = [tensorboardCallback2])\n",
    "    model3.fit(X, y, epochs = 100, validation_data = (x_test, y_test), callbacks = [tensorboardCallback3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c448e184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.6019e-07 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 327ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.5660 - val_loss: 1.4088e-07 - val_accuracy: 0.6364\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 231ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.5274e-07 - accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5295e-07 - accuracy: 0.0755 - val_loss: 1.4088e-07 - val_accuracy: 0.0909\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 238ms/step - loss: 0.0000e+00 - accuracy: 0.3585 - val_loss: 0.0000e+00 - val_accuracy: 0.6364\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nic12\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.5660 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: nan - accuracy: 0.0755 - val_loss: nan - val_accuracy: 0.0909\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "train_models2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9f1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 11736), started 0:03:58 ago. (Use '!kill 11736' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9ede3407f0b35a28\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9ede3407f0b35a28\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir logs2\n",
    "%tensorboard --logdir logsAllDenamed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
